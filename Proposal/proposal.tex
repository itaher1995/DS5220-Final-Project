\documentclass[10pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[ruled]{algorithm2e}

\title{Project Proposal}
\author{Ibrahim Taher and Forrest Hooton}
%\date{}							% Activate to display a given date or no dates

\begin{document}
\maketitle
%\section{}
%\subsection{}

\textbf{Abstract.}

Convolutional neural networks (CNN) have demonstrated the capacity to excel at object recognition. However, it fails at generating sequences of meaningful words, such as captions. We aim to accomplish that by adding a second deep learning algorithm into the model, recurrent neural networks (RNN). We propose that the feature selection aspects of CNN can be used as an input of an RNN model to generate relevant captions.

\vspace{3mm}
\textbf{Introduction.}

Computer vision is a field that aims to develop algorithms to create context for digital images or video.$^{1}$ Context, in the case of computer vision problems, means identifiers and/or sequences of words. The explosion of deep learning led to a variety of resources to test these algorithms, such as datasets with images tagged with annotations. There has been ground-breaking work regarding annotating images with simple identifiers. This is accomplished using a convolutional neural network, which is based off of the visual cortex in work done by Hubel and Wiesel.$^2$ A CNN consists of three overarching components: an input layer, hidden layer(s) and an output layer.

Within these hidden layers are various convolution, normalization, fully-connected and pooling layers. Convolution is used to compute the similarity between a feature and a \textit{receptive field} via element-wise multiplication.$^3$ The output for the convolution step is then normalized using the ReLU function.$^4$ Afterwards, pooling occurs. In practice, pooling is taking the max value of a given window in our normalized convoluted data. The window then slides by a given window stride rate. This effort is done to reduce the dimensionality of the data. Finally, a fully connected layer is a flattened vector of our reduced dimensionality image which allows for each value of the vector to be given a "vote" as to the classification of the image.$^5$

While convolutional neural networks excel at classifying images using simple identifiers, they lack the capacity to generate captions. Another algorithm, the recurrent neural network, is more adept in solving this problem. A recurrent neural network is an algorithm that has the ability to create sequential data given an input. It does so using the notion of memory. Recurrent neural networks use loops instead of a feed-forward network.$^{6,7}$ 

During each iteration the RNN uses input, output, activation and forgetting gates to handle this notion of memory. These gates are developed via linear combinations of the input vector at epoch t and hidden/cell activation vectors at either epoch t or t-1 depending on the gate. These gates are then transformed using a non-linear function such as the logistic or tanh function, depending on the gate. They handle memory by weighting what outputs in previous epochs are still relevant.$^8$ 

The relevance of image-caption generation models is widespread for today's purposes. Take for instance, those who are visually impaired. If they have the ability to read words at a closer distance, then using this model will allow them to have a better understanding of the context of an image. Also, caption generation is important for all forms of media. Articles that are located in newspapers or on their respective websites are often associated with images depicting events in the article. Image caption generation will remove the need for human supervision and manual creation of captions.$^9$

\vspace{3mm}
\textbf{Proposed Project.}

In this project we aim to combine the convolutional neural network and the recurrent neural network to generate captions that are associated with images. We will do so by first reducing the dimensionality of the data using the CNN, extracting relevant features of the image. These features will then be inputs to our RNN, which over a series of iterations will learn the associations between relevant features and the captions associated with the images. With all of this in mind, we are attempting to implement a \textit{Word From Sequence} model, which given an image and a part of a sequence, the model will try to predict the next word in the sequence.$^{10}$

The model will be tuned using a grid search, in which it will have to be trained on combinations several parameters. For a convolutional neural network, the model will be tuned for the number of features used in convolution, size of windows used and the stride of those windows for max-pooling. For the recurrent neural network a we aim to tune is dropout probability (the probability by which certain neurons don't fire.) For both, learning rate, number of layers and number of neurons will be tuned.

Both algorithms employ a variant of gradient descent known as backpropogation. Since backpropogation for large image datasets, over large intervals is computationally expensive, we will be employing a truncated backpropogation algorithm. Essentially, instead of updated the weight vectors, $\Theta$ at every interval, $\Theta$ will be updated every $p$ epochs.$^{11}$ 

The data comes from the Microsoft COCO 2014 dataset, which includes several thousand images mapped to associated captions. The dataset already comes pre-split with training and validation images but also comes with a testing set. The images are in .jpg format and the annotations are given in JSON format, with mappings to its respective image through an id.$^{12}$

We plan to test our model using the BLEU and ROGUE metrics. BLEU is a method to evaluate the precision of a predicted caption. It is as follows:

\begin{center}
	$BLEU_{(a,b)} = \frac{\sum_{w_n \in a} min(c_a(w_n),max(c_{b_j}(w_n))}{\sum_{w_n \in a} c_a(w_n)}$ $^{13}$
\end{center}

The ROGUE metric is a method to evaluate the recall of a predicted caption. It is as follows:

\begin{center}
	$ROGUE_{(a,b)} = \frac{\sum_{j=1}^{|b|}\sum_{w_n \in b} min(c_a(w_n),c_{b_j}(w_n)}{\sum_{j=1}^{|b|}\sum_{w_n \in b} c_{b_j}(w_n)}$ $^{14}$
\end{center}

For both, $a$ is a predicted caption, $b$ is a set of ground truth captions, $w_n$ is an n-gram and $c_x(y_n)$ is the count of n-gram $y_n$ in caption $c_x$.

Using these metrics, we aim to maximize values returned, thus leading to the best possible predicted captions compared to the ground truth labels. By correctly fitting our data, through training and a grid search on our hyperparameters, we believe this model can generate interesting captions for unseen images.

\vspace{3mm}
\textbf{References.}

$^1$ Huang, T. (1996-11-19). Vandoni, Carlo, E, ed. Computer Vision : Evolution And Promise.

19th CERN School of Computing. Geneva: CERN. pp. 21–25. doi:10.5170/CERN-1996-008.21. 

ISBN 978-9290830955.

\vspace{3mm}
$^2$ Hubel, D. H., and T. N. Wiesel. “Receptive fields and functional architecture of monkey striate 

cortex.” The Journal of Physiology, vol. 195, no. 1, Jan. 1968, pp. 215–243., doi:10.1113/jphysiol.1968.sp008455. 

\vspace{3mm}
$^3$ B. B. Le Cun, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, 

Handwritten digit recognition with a back-propagation network, in:  Proceedings of the Advances

in Neural Information Processing Systems (NIPS), 1989


\vspace{3mm}
$^4$An Intuitive Explanation of Convolutional Neural Networks. (2017, May 29).

Retrieved March 11, 2018, from https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/

\vspace{3mm}
$^5$ Rohrer, Brandon. “How Convolutional Neural Networks Work?” Brandon Rohrer, 2017, 

brohrer.github.io/how\_convolutional\_neural\_networks\_work.html 

\vspace{3mm}
$^6$ Karpathy, Andrey. "The Unreasonable Effectiveness of Recurrent Neural Networks"

Andrey Karpathy, 2015-11-05, https://karpathy.github.io/2015/05/21/rnn-effectiveness/

\vspace{3mm}
$^7$ Britz, Denny. "Recurrent Neural Networks Tutorial, Part 1 - Introduction to RNNs" WildML,

2015-17-09, 
http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/

\vspace{3mm}
$^8$ Graves, Alex, et al. “Speech recognition with deep recurrent neural networks.” 2013 IEEE 

International Conference on Acoustics, Speech and Signal Processing, 2013, doi:10.1109/icassp.2013.6638947. 

\vspace{3mm}
$^9$ Kiruthika, N P, et al. "EXTRACTIVE AND ABSTRACTIVE CAPTION GENERATION 

MODEL FOR NEWS IMAGES." International Journal of Innovative Research in Technology 

\& Science(IJIRTS), ijirts.org/volume2issue2/IJIRTSV2I2060.pdf

\vspace{3mm}
$^{10}$ Brownlee, Jason. "A Gentle Introduction to Deep Learning Caption Generation Models",

Machine Learning Mastery, 2017-17-11,
https://machinelearningmastery.com/deep-learning-caption-generation-models/

\vspace{3mm}
$^{11}$ "A Gentle Introduction to Backpropagation Through Time." Machine Learning Mastery, 

19 July 2017, machinelearningmastery.com/gentle-introduction-backpropagation-time/. 

\vspace{3mm}
$^{12}$ COCO - Common Objects in Context, http://cocodataset.org/\#download

\vspace{3mm}
$^{13}$ Papineni et. al., BLEU: A Method for Automatic Evaluation of Machine Translation, 200

\vspace{3mm}
$^{14}$ Lin et. al., ROUGE: A Package for Automatic Evaluation of Summaries, 200



\end{document}



